<!doctype html>

<html>
<head>
    <meta charset="utf-8">
    <title>WebGPU Life</title>
</head>
<body>
<canvas width="512" height="512"></canvas>
<script type="module">
    // Your WebGPU code will begin here!

    const GRID_SIZE = 32;

    // check if webgpu is supported
    if (!navigator.gpu) {
        throw new Error("WebGPU not supported on this browser!")
    }
    //  check if gpu is available
    const adapter = await navigator.gpu.requestAdapter()
    if (!adapter) {
        throw new Error("No appropriate GPU found.")
    }
    // request a gpu
    const device = await adapter.requestDevice();

    // get canvas element and get webgpu context
    const canvas = document.querySelector("canvas");
    const context = canvas.getContext("webgpu");
    // returns optimal texture format
    const canvasFormat = navigator.gpu.getPreferredCanvasFormat();
    // context returned by canvas must be configured
    context.configure({device, format: canvasFormat});

    // encoder is an interface that records gpu commands
    const encoder = device.createCommandEncoder();

    const vertices = new Float32Array([
//   X,    Y,
        -0.8, -0.8, // Triangle 1 (Blue)
        0.8, -0.8,
        0.8, 0.8,

        -0.8, -0.8, // Triangle 2 (Red)
        0.8, 0.8,
        -0.8, 0.8,
    ]);

    // create a uniform buffer for the grid
    const uniformArray = new Float32Array([GRID_SIZE, GRID_SIZE]);
    const uniformBuffer = device.createBuffer({
        label: "Grid Uniforms",
        size: uniformArray.byteLength,
        usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
    });
    device.queue.writeBuffer(uniformBuffer, 0, uniformArray);

    // create a gpu buffer for the vertices
    const vertexBuffer = device.createBuffer({
        // optional label for each buffer
        label: "Cell vertices",
        // bytes of memory to be allocated
        size: vertices.byteLength,
        // bitwise flags to be passed for usage of buffer
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST
    });
    // copy data into device buffer memory
    device.queue.writeBuffer(vertexBuffer, 0, vertices);

    // define the vertex layout
    const vertexBufferLayout = {
        // bytes in an element (float32x2 => 32*2 = 8 * (4 * 2) )
        arrayStride: 4 * 2,
        // info regarding each element (here, a vertex)
        attributes: [{
            // for available formats check GPUVertexFormat
            format: "float32x2",
            // bytes to skip in the element to get to the attribute
            offset: 0,
            // pointer to location of variable in shader (between 0 and 15)
            shaderLocation: 0
        }]
    }

    // shaders
    const cellShaderModule = device.createShaderModule({
        label: "Cell Shader",
        code: `
        // shader code
        
        // WSGL runs shader functions in parallel, meaning vertices are processed non-sequentially.
        // Each vertex shader functions receives a single vertex (from the buffer) as an argument,
        // and produces the output for a single vertex.
        
        // using structs to pass data from vertex shader to fragment shader
        struct VtxIn{
            @location(0) vtxPos: vec2f,
            @builtin(instance_index) instance: u32
        };
        
        struct VtxOut{
            @builtin(position) vtxPos: vec4f,
            @location(0) cell: vec2f
        };
        
        // an alternate method when shader and fragment code is not in the same place
        struct FragIn{
            @location(0) cell: vec2f
        };
        
        // declaring grid uniform
        @group(0) @binding(0) var<uniform> grid: vec2f;
        
        // The type of a function (declared by 'fn') is defined by the attribute before it.
        // for vertex shader, it is '@vertex', and this function MUST return the position of the
        // transformed vertex by setting the built-in attribute 'position' to the final value.
        
        // The vertex shader is responsible for transforming the world coordinates to clip space,
        // and passes them to the rasterizer, which is responsible for finding the pixels within
        // the transformed triangle in the clip space. However, the pixels have not yet been colored.
        
        @vertex
        fn vertexMain(vtxData: VtxIn) ->
        VtxOut {
            let i = f32(vtxData.instance);
            let cell = vec2f(floor(i / grid.x), i % grid.y);
            let cellOffset = (cell / grid) * 2;
            let gridPos = ((vtxData.vtxPos + 1) / grid) - 1 + cellOffset;
            
            var output: VtxOut;
            output.vtxPos = vec4f(gridPos, 0, 1);
            output.cell = cell;
            return output;
        }
        
        // The fragment shader is responsible for coloring each pixel that the rasterizer picks.
        // The fragment shader returns the color the pixel should be colored
        
        @fragment
        fn fragmentMain(vtxData: VtxOut) -> @location(0) vec4f {
            var c = vtxData.cell / grid;
            return vec4f(c, 1-c.x, 1);
        }
        `
    })

    // render pipeline
    const cellPipeline = device.createRenderPipeline({
        label: "Cell pipeline",
        // what types of input the pipeline needs
        layout: "auto",
        // vertex stage definition
        vertex: {
            // where shader code is defined
            module: cellShaderModule,
            // name of function that should be run first
            entryPoint: "vertexMain",
            // how data is packed into your buffers
            buffers: [vertexBufferLayout]
        },
        fragment: {
            module: cellShaderModule,
            entryPoint: "fragmentMain",
            // details of the output format
            targets: [{
                // must match textures given in colorAttachments
                format: canvasFormat
            }]
        }
    });

    // to bind the uniform buffer to the shader
    const bindGroup = device.createBindGroup({
        label: "Cell renderer bind group",
        // '0' corresponds to the @group(0) in the shader
        layout: cellPipeline.getBindGroupLayout(0),
        entries: [{
            // '0' corresponds to the @binding(0) in the shader
            binding: 0,
            // where the actual resource (buffer) is located
            resource: {buffer: uniformBuffer}
        }]
    })

    // All render passes start with a `beginRenderPass()` call.
    // It defines the textures that receive output of a command.
    // begin recording commands
    const pass = encoder.beginRenderPass({
        colorAttachments: [{
            // defining "where" (which view) the rendered output must go to
            view: context.getCurrentTexture().createView(),
            // what to do when render pass starts
            loadOp: "clear",
            // set clear value
            clearValue: [0, 0, 0.4, 0.1],
            // what to do when render pass ends
            storeOp: "store"
        }]
    })

    // set which pipeline should be used
    pass.setPipeline(cellPipeline)
    // set the buffer that contains the vertices
    pass.setVertexBuffer(0, vertexBuffer)       // 0 for the index of the element in vertexBuffer
    
    // bind grid size uniform
    pass.setBindGroup(0, bindGroup);
    
    // draw triangle for n vertices
    pass.draw(vertices.length / 2, GRID_SIZE * GRID_SIZE)

    // stop recording commands
    pass.end()

    // // create a command buffer
    // const commandBuffer = encoder.finish();
    // // submit the command buffer to the gpu device's queue
    // device.queue.submit([commandBuffer]);

    // // commandBuffer is now useless, so instead, this is often done:
    device.queue.submit([encoder.finish()])

</script>
</body>
</html>